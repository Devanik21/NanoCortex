import streamlit as st
import PyPDF2
import numpy as np
import re
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Set page config
st.set_page_config(
    page_title="PDF Tiny Language Model",
    page_icon="ğŸ§ ",
    layout="wide"
)

class TextDataset(Dataset):
    def __init__(self, sequences):
        self.sequences = sequences
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        return torch.tensor(sequence[:-1], dtype=torch.long), torch.tensor(sequence[-1], dtype=torch.long)

class TinyLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2):
        super(TinyLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output

class TinyLanguageModel:
    def __init__(self):
        self.model = None
        self.vocab_size = 0
        self.max_sequence_length = 30
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.text_chunks = []
        self.is_trained = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def extract_text_from_pdf(self, pdf_file):
        """Extract text from uploaded PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            st.error(f"Error reading PDF: {str(e)}")
            return None
    
    def preprocess_text(self, text):
        """Clean and preprocess text"""
        # Clean text
        text = re.sub(r'[^\w\s\.\,\?\!\:\;\-]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.lower().strip()
        
        # Split into chunks for training
        sentences = re.split(r'[.!?]+', text)
        self.text_chunks = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        return text
    
    def build_vocabulary(self, text):
        """Build vocabulary from text"""
        words = text.split()
        word_counts = Counter(words)
        
        # Keep most common words
        most_common = word_counts.most_common(1500)
        
        # Build word-to-index mapping
        self.word_to_idx = {'<UNK>': 0, '<START>': 1, '<END>': 2}
        self.idx_to_word = {0: '<UNK>', 1: '<START>', 2: '<END>'}
        
        for i, (word, _) in enumerate(most_common):
            self.word_to_idx[word] = i + 3
            self.idx_to_word[i + 3] = word
        
        self.vocab_size = len(self.word_to_idx)
        
    def text_to_sequences(self, text):
        """Convert text to sequences of token indices"""
        words = text.split()
        sequences = []
        
        for i in range(len(words) - self.max_sequence_length):
            sequence = []
            for j in range(i, i + self.max_sequence_length + 1):
                word = words[j] if j < len(words) else '<END>'
                sequence.append(self.word_to_idx.get(word, 0))
            sequences.append(sequence)
        
        return sequences
    
    def train_model(self, text, epochs=15, batch_size=32, learning_rate=0.001):
        """Train the language model"""
        # Preprocess text and build vocabulary
        clean_text = self.preprocess_text(text)
        self.build_vocabulary(clean_text)
        
        if self.vocab_size < 50:
            return False, "Not enough vocabulary to train model"
        
        # Create sequences
        sequences = self.text_to_sequences(clean_text)
        
        if len(sequences) < 20:
            return False, "Not enough training data"
        
        # Create dataset and dataloader
        dataset = TextDataset(sequences)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Initialize model
        self.model = TinyLSTM(self.vocab_size).to(self.device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        # Training loop
        losses = []
        accuracies = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            correct = 0
            total = 0
            
            for batch_x, batch_y in dataloader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            avg_loss = epoch_loss / len(dataloader)
            accuracy = 100 * correct / total
            
            losses.append(avg_loss)
            accuracies.append(accuracy)
        
        self.is_trained = True
        return True, {'losses': losses, 'accuracies': accuracies}
    
    def generate_text(self, seed_text, max_length=50, temperature=0.8):
        """Generate text using the trained model"""
        if not self.is_trained or not self.model:
            return "Model is not trained yet!"
        
        self.model.eval()
        
        # Prepare seed
        words = seed_text.lower().split()
        sequence = []
        
        for word in words[-self.max_sequence_length:]:
            sequence.append(self.word_to_idx.get(word, 0))
        
        # Pad if necessary
        while len(sequence) < self.max_sequence_length:
            sequence.insert(0, 0)
        
        generated = words.copy()
        
        with torch.no_grad():
            for _ in range(max_length):
                # Predict next word
                x = torch.tensor([sequence], dtype=torch.long).to(self.device)
                output = self.model(x)
                
                # Apply temperature
                output = output / temperature
                probabilities = F.softmax(output, dim=-1)
                
                # Sample from distribution
                next_idx = torch.multinomial(probabilities, 1).item()
                next_word = self.idx_to_word.get(next_idx, '<UNK>')
                
                if next_word in ['<END>', '<UNK>'] or len(generated) > max_length:
                    break
                    
                generated.append(next_word)
                
                # Update sequence
                sequence = sequence[1:] + [next_idx]
        
        return ' '.join(generated)
    
    def answer_question(self, question):
        """Answer questions using both retrieval and generation"""
        if not self.text_chunks:
            return "No document loaded!"
        
        # Simple retrieval first
        question_words = set(question.lower().split())
        best_chunk = ""
        best_score = 0
        
        for chunk in self.text_chunks:
            chunk_words = set(chunk.lower().split())
            overlap = len(question_words.intersection(chunk_words))
            score = overlap + len([w for w in question_words if w in chunk.lower()])
            if score > best_score:
                best_score = score
                best_chunk = chunk
        
        if best_score > 0:
            # Use the best chunk as context and generate response
            if self.is_trained and self.model:
                context = best_chunk[:100]  # First part as seed
                generated = self.generate_text(context, max_length=40)
                return f"**Based on the document:**\n\n{generated}"
            else:
                return f"**Found relevant information:**\n\n{best_chunk}"
        else:
            if self.is_trained and self.model:
                # Generate based on question keywords
                seed = ' '.join(list(question_words)[:5])
                generated = self.generate_text(seed, max_length=40)
                return f"**AI Generated Response:**\n\n{generated}"
            else:
                return "Could not find relevant information in the document."

# Initialize the model
if 'tlm' not in st.session_state:
    st.session_state.tlm = TinyLanguageModel()
    st.session_state.document_loaded = False
    st.session_state.model_trained = False

# App header
st.title("ğŸ§  PDF Tiny Language Model (PyTorch)")
st.markdown("Upload a PDF and train a small neural language model on its content!")

# Sidebar for PDF upload and training
with st.sidebar:
    st.header("ğŸ“ Document & Training")
    
    uploaded_file = st.file_uploader(
        "Choose a PDF file",
        type="pdf",
        help="Upload a PDF document to train the model"
    )
    
    if uploaded_file is not None:
        if st.button("ğŸ“„ Process PDF", type="primary"):
            with st.spinner("Extracting text from PDF..."):
                text = st.session_state.tlm.extract_text_from_pdf(uploaded_file)
                
                if text and len(text) > 100:
                    st.session_state.tlm.raw_text = text
                    st.session_state.document_loaded = True
                    st.success("âœ… PDF processed successfully!")
                    st.info(f"ğŸ“Š Text length: {len(text):,} characters")
                    
                    # Show text preview
                    with st.expander("ğŸ“– Text Preview"):
                        st.text(text[:500] + "..." if len(text) > 500 else text)
                else:
                    st.error("Could not extract sufficient text from PDF")
    
    if st.session_state.document_loaded and not st.session_state.model_trained:
        st.header("ğŸ§  Train Model")
        
        col1, col2 = st.columns(2)
        with col1:
            epochs = st.slider("Epochs", 5, 100, 30)
        with col2:
            batch_size = st.selectbox("Batch Size", [4,8,16, 32, 64], index=1)
        
        learning_rate = st.select_slider(
            "Learning Rate", 
            options=[0.01, 0.005, 0.001, 0.0005], 
            value=0.001,
            format_func=lambda x: f"{x:.4f}"
        )
        
        if st.button("ğŸš€ Train Language Model", type="primary"):
            with st.spinner(f"Training model for {epochs} epochs..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # Simulate progress updates
                for i in range(epochs):
                    progress_bar.progress((i + 1) / epochs)
                    status_text.text(f"Epoch {i+1}/{epochs}")
                
                success, result = st.session_state.tlm.train_model(
                    st.session_state.tlm.raw_text, 
                    epochs=epochs,
                    batch_size=batch_size,
                    learning_rate=learning_rate
                )
                
                if success:
                    st.session_state.model_trained = True
                    st.success("ğŸ‰ Model trained successfully!")
                    
                    # Show training stats
                    final_loss = result['losses'][-1]
                    final_acc = result['accuracies'][-1]
                    st.metric("Final Loss", f"{final_loss:.3f}")
                    st.metric("Final Accuracy", f"{final_acc:.1f}%")
                    
                    # Plot training progress
                    import matplotlib.pyplot as plt
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
                    
                    ax1.plot(result['losses'])
                    ax1.set_title('Training Loss')
                    ax1.set_xlabel('Epoch')
                    ax1.set_ylabel('Loss')
                    
                    ax2.plot(result['accuracies'])
                    ax2.set_title('Training Accuracy')
                    ax2.set_xlabel('Epoch')
                    ax2.set_ylabel('Accuracy (%)')
                    
                    st.pyplot(fig)
                else:
                    st.error(f"Training failed: {result}")

# Main content area
col1, col2 = st.columns([3, 2])

with col1:
    if st.session_state.model_trained:
        st.header("ğŸ¤– AI Assistant")
        st.success("ğŸ§  Neural model is trained and ready!")
        
        # Quick action buttons
        st.subheader("âš¡ Quick Actions")
        col_a, col_b, col_c = st.columns(3)
        
        with col_a:
            if st.button("ğŸ“‹ Summarize"):
                with st.spinner("Generating summary..."):
                    answer = st.session_state.tlm.answer_question("summarize main points key information")
                    st.write(answer)
        
        with col_b:
            if st.button("ğŸ” Main Topic"):
                with st.spinner("Finding main topic..."):
                    answer = st.session_state.tlm.answer_question("what is this document about main topic")
                    st.write(answer)
        
        with col_c:
            if st.button("ğŸ’¡ Key Points"):
                with st.spinner("Extracting key points..."):
                    answer = st.session_state.tlm.answer_question("important details key facts")
                    st.write(answer)
        
        st.markdown("---")
        
        # Question answering
        st.subheader("â“ Ask Questions")
        question = st.text_area(
            "Ask anything about the document:",
            placeholder="What are the main conclusions?",
            height=80
        )
        
        if st.button("ğŸ” Get Answer") and question:
            with st.spinner("Generating answer..."):
                answer = st.session_state.tlm.answer_question(question)
                st.write(answer)
        
        st.markdown("---")
        
        # Text generation
        st.subheader("âœ¨ Generate Text")
        seed_text = st.text_input(
            "Enter seed text:",
            placeholder="The main findings show that..."
        )
        
        col_gen1, col_gen2 = st.columns(2)
        with col_gen1:
            max_length = st.slider("Max Length", 20, 100, 50)
        with col_gen2:
            temperature = st.slider("Creativity", 0.3, 1.5, 0.8, 0.1)
        
        if st.button("ğŸ“ Generate") and seed_text:
            with st.spinner("Generating text..."):
                generated = st.session_state.tlm.generate_text(
                    seed_text, 
                    max_length=max_length,
                    temperature=temperature
                )
                st.subheader("ğŸ¯ Generated Text:")
                st.write(generated)
    
    elif st.session_state.document_loaded:
        st.info("ğŸ“„ Document loaded! Now train the language model using the sidebar.")
        
        # Show document stats
        if hasattr(st.session_state.tlm, 'raw_text'):
            text = st.session_state.tlm.raw_text
            st.subheader("ğŸ“Š Document Statistics")
            
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                st.metric("Characters", f"{len(text):,}")
            with col_stat2:
                st.metric("Words", f"{len(text.split()):,}")
            with col_stat3:
                st.metric("Sentences", f"{len(re.split(r'[.!?]+', text)):,}")
    else:
        st.info("ğŸ‘ˆ Upload a PDF document first to get started.")

with col2:
    st.header("â„¹ï¸ How it Works")
    
    if st.session_state.model_trained:
        st.markdown("""
        **ğŸ§  Neural Language Model Trained!**
        
        **Architecture:**
        - ğŸ”¤ Embedding Layer (64D)
        - ğŸ§  2x LSTM Layers (128 units)
        - ğŸ¯ Dense Output Layer
        - ğŸ“Š Vocabulary: {} words
        
        **Training Complete:**
        - âœ… Learned word patterns
        - âœ… Sequence relationships
        - âœ… Document-specific style
        - âœ… Ready for inference
        """.format(st.session_state.tlm.vocab_size))
        
        st.header("ğŸ“Š Model Stats")
        if hasattr(st.session_state.tlm, 'vocab_size'):
            st.metric("Vocabulary", st.session_state.tlm.vocab_size)
            st.metric("Sequence Length", st.session_state.tlm.max_sequence_length)
            st.metric("Text Chunks", len(st.session_state.tlm.text_chunks))
            st.metric("Device", str(st.session_state.tlm.device).upper())
    
    else:
        st.markdown("""
        **ğŸš€ PyTorch Neural Network:**
        
        **Training Process:**
        1. ğŸ“„ Extract text from PDF
        2. ğŸ”¤ Build vocabulary & tokenize
        3. ğŸ“Š Create training sequences
        4. ğŸ§  Train LSTM neural network
        5. ğŸ¤– Generate & answer questions
        
        **Model Features:**
        - ğŸ”¥ PyTorch-based (lightweight)
        - ğŸ§  LSTM architecture
        - ğŸ“ˆ Real-time training metrics
        - ğŸ¯ Temperature-controlled generation
        - ğŸ’¾ CPU/GPU support
        
        **No heavy dependencies!**
        """)

# Footer
st.markdown("---")
st.markdown("ğŸ§  **PyTorch Tiny Language Model** â€¢ Real neural network training in Streamlit!")
